#!/usr/bin/env python3
"""
Test AI Question Generation and Scoring
Run this to verify AI features are working
"""

import os
from dotenv import load_dotenv

load_dotenv()

print("="*80)
print("ü§ñ TESTING AI FEATURES")
print("="*80)

# Test 1: Check Gemini API Key
print("\n[Test 1] Checking Gemini API Key...")
api_key = os.getenv('GEMINI_API_KEY')

if not api_key:
    print("‚ùå GEMINI_API_KEY not found in .env file")
    print("\nFIX: Add this to your .env file:")
    print("GEMINI_API_KEY=your_actual_api_key_here")
    print("\nGet key from: https://aistudio.google.com/app/apikey")
    exit(1)
else:
    print(f"‚úÖ API Key found: {api_key[:20]}...")

# Test 2: Test AI Question Generation
print("\n[Test 2] Testing AI Question Generation...")

try:
    from ai_question_generator import generate_quiz_questions
    
    print("Generating 3 sample questions for C - Arrays (beginner level)...")
    questions = generate_quiz_questions('C', 'Arrays', 'beginner', 3)
    
    if len(questions) >= 3:
        print(f"‚úÖ Generated {len(questions)} questions successfully!")
        
        # Display first question as sample
        print("\nüìù Sample Question:")
        print("-"*80)
        q = questions[0]
        print(f"Q: {q['q']}")
        print(f"\nOptions:")
        for i, opt in enumerate(q['options'], 1):
            marker = " ‚Üê CORRECT" if opt == q['answer'] else ""
            print(f"  {i}. {opt}{marker}")
        print(f"\nCorrect Answer: '{q['answer']}'")
        print(f"Answer is in options: {q['answer'] in q['options']}")
        print("-"*80)
        
    else:
        print(f"‚ö†Ô∏è  Only generated {len(questions)} questions")
        
except Exception as e:
    print(f"‚ùå AI generation failed: {e}")
    print("\nPossible issues:")
    print("1. Invalid GEMINI_API_KEY")
    print("2. No internet connection")
    print("3. API quota exceeded")
    exit(1)

# Test 3: Test Scoring Logic
print("\n[Test 3] Testing Scoring Logic...")

test_cases = [
    {
        'user_answer': 'O(log n)',
        'correct_answer': 'O(log n)',
        'expected': True,
        'name': 'Exact match'
    },
    {
        'user_answer': 'o(log n)',
        'correct_answer': 'O(log n)',
        'expected': True,
        'name': 'Case insensitive'
    },
    {
        'user_answer': '  O(log n)  ',
        'correct_answer': 'O(log n)',
        'expected': True,
        'name': 'Extra spaces'
    },
    {
        'user_answer': 'O(n)',
        'correct_answer': 'O(log n)',
        'expected': False,
        'name': 'Wrong answer'
    }
]

all_passed = True

for test in test_cases:
    # Apply same logic as app.py scoring
    user_normalized = test['user_answer'].strip().lower()
    correct_normalized = test['correct_answer'].strip().lower()
    is_correct = user_normalized == correct_normalized
    
    if is_correct == test['expected']:
        print(f"  ‚úÖ {test['name']}: PASS")
    else:
        print(f"  ‚ùå {test['name']}: FAIL")
        all_passed = False

if all_passed:
    print("\n‚úÖ All scoring tests passed!")
else:
    print("\n‚ö†Ô∏è  Some scoring tests failed")

# Test 4: Check if app.py imports work
print("\n[Test 4] Checking app.py imports...")

try:
    from app import app, supabase
    print("‚úÖ Flask app imports successfully")
    print("‚úÖ Supabase client initialized")
except Exception as e:
    print(f"‚ùå Import error: {e}")
    exit(1)

# Test 5: Check ML model
print("\n[Test 5] Checking ML Suggestion Model...")

try:
    from ml_model import analyze_user_performance, get_overall_readiness
    
    # Test with sample data
    sample_scores = [
        {'topic': 'C', 'subtopic': 'Arrays', 'score': 3, 'total_questions': 5},
        {'topic': 'Java', 'subtopic': 'OOPs', 'score': 4, 'total_questions': 5},
    ]
    
    suggestions = analyze_user_performance(sample_scores)
    readiness = get_overall_readiness(sample_scores)
    
    print(f"‚úÖ ML model working - Generated {len(suggestions)} suggestions")
    print(f"‚úÖ Readiness score: {readiness['score']}%")
    
except Exception as e:
    print(f"‚ö†Ô∏è  ML model issue: {e}")
    print("   This won't prevent the app from running")

# Final Report
print("\n" + "="*80)
print("üìä FINAL REPORT")
print("="*80)

print("\n‚úÖ AI Features Status:")
print("   ‚úÖ Gemini API Key: Configured")
print("   ‚úÖ Question Generation: Working")
print("   ‚úÖ Scoring Logic: Working")
print("   ‚úÖ App Imports: Working")

print("\nüéØ What This Means:")
print("   ‚Ä¢ AI will generate adaptive questions based on user performance")
print("   ‚Ä¢ Questions adapt difficulty (beginner ‚Üí intermediate ‚Üí advanced)")
print("   ‚Ä¢ Scoring validates answers correctly")
print("   ‚Ä¢ ML suggestions analyze performance patterns")

print("\nüöÄ How to Use:")
print("   1. Run: python app.py")
print("   2. Register/Login at http://localhost:5000")
print("   3. Take a quiz - you'll get AI-generated questions")
print("   4. Questions will adapt to your skill level")
print("   5. Check 'Suggestions' for ML-powered recommendations")

print("\nüí° Tips:")
print("   ‚Ä¢ First quiz = Beginner level")
print("   ‚Ä¢ Score 75%+ to unlock Intermediate")
print("   ‚Ä¢ Score 75%+ on Intermediate to unlock Advanced")
print("   ‚Ä¢ Each topic/subtopic adapts independently")

print("\n" + "="*80)
print("‚úÖ ALL SYSTEMS GO! Your AI features are working!")
print("="*80)